{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.29.0)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.13.3)\n",
      "Requirement already satisfied: webdriver-manager in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: python-dotenv in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium requests tqdm beautifulsoup4 webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-08 04:26:43--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
      "Resolving dl.google.com (dl.google.com)... 142.251.179.136, 142.251.179.93, 142.251.179.190, ...\n",
      "Connecting to dl.google.com (dl.google.com)|142.251.179.136|:443... connected.\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "Length: 114757600 (109M) [application/x-debian-package]\n",
      "Saving to: ‘google-chrome-stable_current_amd64.deb.2’\n",
      "\n",
      "google-chrome-stabl 100%[===================>] 109.44M   262MB/s    in 0.4s    \n",
      "\n",
      "2025-03-08 04:26:44 (262 MB/s) - ‘google-chrome-stable_current_amd64.deb.2’ saved [114757600/114757600]\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Note, selecting 'google-chrome-stable' instead of './google-chrome-stable_current_amd64.deb'\n",
      "The following additional packages will be installed:\n",
      "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
      "  libfile-basedir-perl libfile-desktopentry-perl libfile-listing-perl\n",
      "  libfile-mimeinfo-perl libfont-afm-perl libfontenc1 libhtml-form-perl\n",
      "  libhtml-format-perl libhtml-parser-perl libhtml-tagset-perl\n",
      "  libhtml-tree-perl libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
      "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
      "  libio-socket-ssl-perl libio-stringy-perl libipc-system-simple-perl\n",
      "  liblwp-mediatypes-perl liblwp-protocol-https-perl libmailtools-perl\n",
      "  libnet-dbus-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n",
      "  libnspr4 libnss3 libtext-iconv-perl libtie-ixhash-perl libtimedate-perl\n",
      "  libtry-tiny-perl liburi-perl libwww-perl libwww-robotrules-perl\n",
      "  libx11-protocol-perl libxkbfile1 libxml-parser-perl libxml-twig-perl\n",
      "  libxml-xpathengine-perl libxv1 libxxf86dga1 perl-openssl-defaults x11-utils\n",
      "  x11-xserver-utils xdg-utils\n",
      "Suggested packages:\n",
      "  libdigest-hmac-perl libgssapi-perl libcrypt-ssleay-perl libauthen-ntlm-perl\n",
      "  libunicode-map8-perl libunicode-string-perl xml-twig-tools mesa-utils nickle\n",
      "  cairo-5c xorg-docs-core\n",
      "The following NEW packages will be installed:\n",
      "  google-chrome-stable libauthen-sasl-perl libdata-dump-perl\n",
      "  libencode-locale-perl libfile-basedir-perl libfile-desktopentry-perl\n",
      "  libfile-listing-perl libfile-mimeinfo-perl libfont-afm-perl libfontenc1\n",
      "  libhtml-form-perl libhtml-format-perl libhtml-parser-perl\n",
      "  libhtml-tagset-perl libhtml-tree-perl libhttp-cookies-perl\n",
      "  libhttp-daemon-perl libhttp-date-perl libhttp-message-perl\n",
      "  libhttp-negotiate-perl libio-html-perl libio-socket-ssl-perl\n",
      "  libio-stringy-perl libipc-system-simple-perl liblwp-mediatypes-perl\n",
      "  liblwp-protocol-https-perl libmailtools-perl libnet-dbus-perl\n",
      "  libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl libnspr4 libnss3\n",
      "  libtext-iconv-perl libtie-ixhash-perl libtimedate-perl libtry-tiny-perl\n",
      "  liburi-perl libwww-perl libwww-robotrules-perl libx11-protocol-perl\n",
      "  libxkbfile1 libxml-parser-perl libxml-twig-perl libxml-xpathengine-perl\n",
      "  libxv1 libxxf86dga1 perl-openssl-defaults x11-utils x11-xserver-utils\n",
      "  xdg-utils\n",
      "0 upgraded, 51 newly installed, 0 to remove and 5 not upgraded.\n",
      "Need to get 4431 kB/119 MB of archives.\n",
      "After this operation, 389 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libnspr4 amd64 2:4.35-0ubuntu0.20.04.1 [108 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libnss3 amd64 2:3.98-0ubuntu0.20.04.2 [1391 kB]\n",
      "Get:3 /teamspace/studios/this_studio/google-chrome-stable_current_amd64.deb google-chrome-stable amd64 134.0.6998.35-1 [115 MB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 xdg-utils all 1.1.3-2ubuntu1.20.04.2 [61.4 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libtext-iconv-perl amd64 1.7-7 [13.8 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libdata-dump-perl all 1.23-1 [27.0 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libipc-system-simple-perl all 1.26-1 [22.8 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libfile-basedir-perl all 0.08-1 [16.9 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 liburi-perl all 1.76-2 [77.5 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libfile-desktopentry-perl all 0.22-1 [18.2 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libtimedate-perl all 2.3200-1 [34.0 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 libhttp-date-perl all 6.05-1 [9920 B]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 libfile-listing-perl all 6.04-1 [9774 B]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 libfile-mimeinfo-perl all 0.29-1 [41.5 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 libfont-afm-perl all 1.20-2 [13.2 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 libfontenc1 amd64 1:1.1.4-0ubuntu1 [14.0 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal/main amd64 libhtml-tagset-perl all 3.20-4 [12.5 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal/main amd64 libhtml-parser-perl amd64 3.72-5 [86.3 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal/main amd64 liblwp-mediatypes-perl all 6.04-1 [19.5 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal/main amd64 libhttp-message-perl all 6.22-1 [76.1 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu focal/main amd64 libhtml-form-perl all 6.07-1 [22.2 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu focal/main amd64 libhtml-tree-perl all 5.07-2 [200 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu focal/main amd64 libhtml-format-perl all 2.12-1 [41.3 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu focal/main amd64 libhttp-cookies-perl all 6.08-1 [18.3 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libhttp-daemon-perl all 6.06-1ubuntu0.1 [22.0 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu focal/main amd64 libhttp-negotiate-perl all 6.01-1 [12.5 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu focal/main amd64 perl-openssl-defaults amd64 4 [7192 B]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu focal/main amd64 libnet-ssleay-perl amd64 1.88-2ubuntu1 [291 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu focal/main amd64 libio-socket-ssl-perl all 2.067-1 [176 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu focal/main amd64 libio-stringy-perl all 2.111-3 [55.8 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu focal/main amd64 libnet-http-perl all 6.19-1 [22.8 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu focal/main amd64 libtry-tiny-perl all 0.30-1 [20.5 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu focal/main amd64 libwww-robotrules-perl all 6.02-1 [12.6 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu focal/main amd64 libwww-perl all 6.43-1 [140 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu focal/main amd64 liblwp-protocol-https-perl all 6.07-2ubuntu2 [8560 B]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu focal/main amd64 libnet-smtp-ssl-perl all 1.04-1 [5948 B]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu focal/main amd64 libmailtools-perl all 2.21-1 [80.7 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu focal/main amd64 libxml-parser-perl amd64 2.46-1 [193 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu focal/main amd64 libxml-twig-perl all 1:3.50-2 [155 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu focal/main amd64 libnet-dbus-perl amd64 1.2.0-1 [177 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu focal/main amd64 libtie-ixhash-perl all 1.23-2 [11.2 kB]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu focal/main amd64 libx11-protocol-perl all 0.56-7 [149 kB]\n",
      "Get:45 http://archive.ubuntu.com/ubuntu focal/main amd64 libxkbfile1 amd64 1:1.1.0-1 [65.3 kB]\n",
      "Get:46 http://archive.ubuntu.com/ubuntu focal/main amd64 libxml-xpathengine-perl all 0.14-1 [31.8 kB]\n",
      "Get:47 http://archive.ubuntu.com/ubuntu focal/main amd64 libxv1 amd64 2:1.0.11-1 [10.7 kB]\n",
      "Get:48 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu1 [12.0 kB]\n",
      "Get:49 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-utils amd64 7.7+5 [199 kB]\n",
      "Get:50 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-xserver-utils amd64 7.7+8 [162 kB]\n",
      "Get:51 http://archive.ubuntu.com/ubuntu focal/main amd64 libauthen-sasl-perl all 2.1600-1 [48.7 kB]\n",
      "Fetched 4431 kB in 1s (3373 kB/s)                \u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libnspr4:amd64.\n",
      "(Reading database ... 69853 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libnspr4_2%3a4.35-0ubuntu0.20.04.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking libnspr4:amd64 (2:4.35-0ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package libnss3:amd64.\n",
      "Preparing to unpack .../01-libnss3_2%3a3.98-0ubuntu0.20.04.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  1%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking libnss3:amd64 (2:3.98-0ubuntu0.20.04.2) ...\n",
      "Selecting previously unselected package xdg-utils.\n",
      "Preparing to unpack .../02-xdg-utils_1.1.3-2ubuntu1.20.04.2_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking xdg-utils (1.1.3-2ubuntu1.20.04.2) ...\n",
      "Selecting previously unselected package google-chrome-stable.\n",
      "Preparing to unpack .../03-google-chrome-stable_current_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  3%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking google-chrome-stable (134.0.6998.35-1) ...\n",
      "Selecting previously unselected package libtext-iconv-perl.\n",
      "Preparing to unpack .../04-libtext-iconv-perl_1.7-7_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Unpacking libtext-iconv-perl (1.7-7) ...\n",
      "Selecting previously unselected package libdata-dump-perl.\n",
      "Preparing to unpack .../05-libdata-dump-perl_1.23-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libdata-dump-perl (1.23-1) ...\n",
      "Selecting previously unselected package libencode-locale-perl.\n",
      "Preparing to unpack .../06-libencode-locale-perl_1.05-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libencode-locale-perl (1.05-1) ...\n",
      "Selecting previously unselected package libipc-system-simple-perl.\n",
      "Preparing to unpack .../07-libipc-system-simple-perl_1.26-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libipc-system-simple-perl (1.26-1) ...\n",
      "Selecting previously unselected package libfile-basedir-perl.\n",
      "Preparing to unpack .../08-libfile-basedir-perl_0.08-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libfile-basedir-perl (0.08-1) ...\n",
      "Selecting previously unselected package liburi-perl.\n",
      "Preparing to unpack .../09-liburi-perl_1.76-2_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking liburi-perl (1.76-2) ...\n",
      "Selecting previously unselected package libfile-desktopentry-perl.\n",
      "Preparing to unpack .../10-libfile-desktopentry-perl_0.22-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking libfile-desktopentry-perl (0.22-1) ...\n",
      "Selecting previously unselected package libtimedate-perl.\n",
      "Preparing to unpack .../11-libtimedate-perl_2.3200-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking libtimedate-perl (2.3200-1) ...\n",
      "Selecting previously unselected package libhttp-date-perl.\n",
      "Preparing to unpack .../12-libhttp-date-perl_6.05-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libhttp-date-perl (6.05-1) ...\n",
      "Selecting previously unselected package libfile-listing-perl.\n",
      "Preparing to unpack .../13-libfile-listing-perl_6.04-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libfile-listing-perl (6.04-1) ...\n",
      "Selecting previously unselected package libfile-mimeinfo-perl.\n",
      "Preparing to unpack .../14-libfile-mimeinfo-perl_0.29-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Unpacking libfile-mimeinfo-perl (0.29-1) ...\n",
      "Selecting previously unselected package libfont-afm-perl.\n",
      "Preparing to unpack .../15-libfont-afm-perl_1.20-2_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Unpacking libfont-afm-perl (1.20-2) ...\n",
      "Selecting previously unselected package libfontenc1:amd64.\n",
      "Preparing to unpack .../16-libfontenc1_1%3a1.1.4-0ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
      "Selecting previously unselected package libhtml-tagset-perl.\n",
      "Preparing to unpack .../17-libhtml-tagset-perl_3.20-4_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking libhtml-tagset-perl (3.20-4) ...\n",
      "Selecting previously unselected package libhtml-parser-perl.\n",
      "Preparing to unpack .../18-libhtml-parser-perl_3.72-5_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libhtml-parser-perl (3.72-5) ...\n",
      "Selecting previously unselected package libio-html-perl.\n",
      "Preparing to unpack .../19-libio-html-perl_1.001-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking libio-html-perl (1.001-1) ...\n",
      "Selecting previously unselected package liblwp-mediatypes-perl.\n",
      "Preparing to unpack .../20-liblwp-mediatypes-perl_6.04-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking liblwp-mediatypes-perl (6.04-1) ...\n",
      "Selecting previously unselected package libhttp-message-perl.\n",
      "Preparing to unpack .../21-libhttp-message-perl_6.22-1_all.deb ...\n",
      "Unpacking libhttp-message-perl (6.22-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libhtml-form-perl.\n",
      "Preparing to unpack .../22-libhtml-form-perl_6.07-1_all.deb ...\n",
      "Unpacking libhtml-form-perl (6.07-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libhtml-tree-perl.\n",
      "Preparing to unpack .../23-libhtml-tree-perl_5.07-2_all.deb ...\n",
      "Unpacking libhtml-tree-perl (5.07-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libhtml-format-perl.\n",
      "Preparing to unpack .../24-libhtml-format-perl_2.12-1_all.deb ...\n",
      "Unpacking libhtml-format-perl (2.12-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [##############............................................] \u001b8Selecting previously unselected package libhttp-cookies-perl.\n",
      "Preparing to unpack .../25-libhttp-cookies-perl_6.08-1_all.deb ...\n",
      "Unpacking libhttp-cookies-perl (6.08-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Selecting previously unselected package libhttp-daemon-perl.\n",
      "Preparing to unpack .../26-libhttp-daemon-perl_6.06-1ubuntu0.1_all.deb ...\n",
      "Unpacking libhttp-daemon-perl (6.06-1ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libhttp-negotiate-perl.\n",
      "Preparing to unpack .../27-libhttp-negotiate-perl_6.01-1_all.deb ...\n",
      "Unpacking libhttp-negotiate-perl (6.01-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package perl-openssl-defaults:amd64.\n",
      "Preparing to unpack .../28-perl-openssl-defaults_4_amd64.deb ...\n",
      "Unpacking perl-openssl-defaults:amd64 (4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package libnet-ssleay-perl.\n",
      "Preparing to unpack .../29-libnet-ssleay-perl_1.88-2ubuntu1_amd64.deb ...\n",
      "Unpacking libnet-ssleay-perl (1.88-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package libio-socket-ssl-perl.\n",
      "Preparing to unpack .../30-libio-socket-ssl-perl_2.067-1_all.deb ...\n",
      "Unpacking libio-socket-ssl-perl (2.067-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package libio-stringy-perl.\n",
      "Preparing to unpack .../31-libio-stringy-perl_2.111-3_all.deb ...\n",
      "Unpacking libio-stringy-perl (2.111-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libnet-http-perl.\n",
      "Preparing to unpack .../32-libnet-http-perl_6.19-1_all.deb ...\n",
      "Unpacking libnet-http-perl (6.19-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libtry-tiny-perl.\n",
      "Preparing to unpack .../33-libtry-tiny-perl_0.30-1_all.deb ...\n",
      "Unpacking libtry-tiny-perl (0.30-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package libwww-robotrules-perl.\n",
      "Preparing to unpack .../34-libwww-robotrules-perl_6.02-1_all.deb ...\n",
      "Unpacking libwww-robotrules-perl (6.02-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package libwww-perl.\n",
      "Preparing to unpack .../35-libwww-perl_6.43-1_all.deb ...\n",
      "Unpacking libwww-perl (6.43-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package liblwp-protocol-https-perl.\n",
      "Preparing to unpack .../36-liblwp-protocol-https-perl_6.07-2ubuntu2_all.deb ...\n",
      "Unpacking liblwp-protocol-https-perl (6.07-2ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package libnet-smtp-ssl-perl.\n",
      "Preparing to unpack .../37-libnet-smtp-ssl-perl_1.04-1_all.deb ...\n",
      "Unpacking libnet-smtp-ssl-perl (1.04-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package libmailtools-perl.\n",
      "Preparing to unpack .../38-libmailtools-perl_2.21-1_all.deb ...\n",
      "Unpacking libmailtools-perl (2.21-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package libxml-parser-perl.\n",
      "Preparing to unpack .../39-libxml-parser-perl_2.46-1_amd64.deb ...\n",
      "Unpacking libxml-parser-perl (2.46-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package libxml-twig-perl.\n",
      "Preparing to unpack .../40-libxml-twig-perl_1%3a3.50-2_all.deb ...\n",
      "Unpacking libxml-twig-perl (1:3.50-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Selecting previously unselected package libnet-dbus-perl.\n",
      "Preparing to unpack .../41-libnet-dbus-perl_1.2.0-1_amd64.deb ...\n",
      "Unpacking libnet-dbus-perl (1.2.0-1) ...\n",
      "Selecting previously unselected package libtie-ixhash-perl.\n",
      "Preparing to unpack .../42-libtie-ixhash-perl_1.23-2_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [########################..................................] \u001b8Unpacking libtie-ixhash-perl (1.23-2) ...\n",
      "Selecting previously unselected package libx11-protocol-perl.\n",
      "Preparing to unpack .../43-libx11-protocol-perl_0.56-7_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Unpacking libx11-protocol-perl (0.56-7) ...\n",
      "Selecting previously unselected package libxkbfile1:amd64.\n",
      "Preparing to unpack .../44-libxkbfile1_1%3a1.1.0-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libxkbfile1:amd64 (1:1.1.0-1) ...\n",
      "Selecting previously unselected package libxml-xpathengine-perl.\n",
      "Preparing to unpack .../45-libxml-xpathengine-perl_0.14-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libxml-xpathengine-perl (0.14-1) ...\n",
      "Selecting previously unselected package libxv1:amd64.\n",
      "Preparing to unpack .../46-libxv1_2%3a1.0.11-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 45%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking libxv1:amd64 (2:1.0.11-1) ...\n",
      "Selecting previously unselected package libxxf86dga1:amd64.\n",
      "Preparing to unpack .../47-libxxf86dga1_2%3a1.1.5-0ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
      "Selecting previously unselected package x11-utils.\n",
      "Preparing to unpack .../48-x11-utils_7.7+5_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Unpacking x11-utils (7.7+5) ...\n",
      "Selecting previously unselected package x11-xserver-utils.\n",
      "Preparing to unpack .../49-x11-xserver-utils_7.7+8_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [############################..............................] \u001b8Unpacking x11-xserver-utils (7.7+8) ...\n",
      "Selecting previously unselected package libauthen-sasl-perl.\n",
      "Preparing to unpack .../50-libauthen-sasl-perl_2.1600-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Unpacking libauthen-sasl-perl (2.1600-1) ...\n",
      "Setting up libtext-iconv-perl (1.7-7) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libtie-ixhash-perl (1.23-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libfont-afm-perl (1.20-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libio-stringy-perl (2.111-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libhtml-tagset-perl (3.20-4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libauthen-sasl-perl (2.1600-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up liblwp-mediatypes-perl (6.04-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libtry-tiny-perl (0.30-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up perl-openssl-defaults:amd64 (4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libencode-locale-perl (1.05-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up x11-xserver-utils (7.7+8) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libdata-dump-perl (1.23-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libnspr4:amd64 (2:4.35-0ubuntu0.20.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libipc-system-simple-perl (1.26-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libxml-xpathengine-perl (0.14-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libxv1:amd64 (2:1.0.11-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libio-html-perl (1.001-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libtimedate-perl (2.3200-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libxkbfile1:amd64 (1:1.1.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up xdg-utils (1.1.3-2ubuntu1.20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up liburi-perl (1.76-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libx11-protocol-perl (0.56-7) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libnet-ssleay-perl (1.88-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libhttp-date-perl (6.05-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libfile-basedir-perl (0.08-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libfile-listing-perl (6.04-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libnet-http-perl (6.19-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libnss3:amd64 (2:3.98-0ubuntu0.20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libfile-desktopentry-perl (0.22-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libwww-robotrules-perl (6.02-1) ...\n",
      "Setting up libhtml-parser-perl (3.72-5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up x11-utils (7.7+5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libio-socket-ssl-perl (2.067-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libhttp-message-perl (6.22-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libhtml-form-perl (6.07-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up libfile-mimeinfo-perl (0.29-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libhttp-negotiate-perl (6.01-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up google-chrome-stable (134.0.6998.35-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [###################################################.......] \u001b8update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
      "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
      "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
      "Setting up libhttp-cookies-perl (6.08-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libhtml-tree-perl (5.07-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libhtml-format-perl (2.12-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libnet-smtp-ssl-perl (1.04-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up libmailtools-perl (2.21-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up libhttp-daemon-perl (6.06-1ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up liblwp-protocol-https-perl (6.07-2ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up libwww-perl (6.43-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up libxml-parser-perl (2.46-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 97%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up libxml-twig-perl (1:3.50-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up libnet-dbus-perl (1.2.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 99%]\u001b[49m\u001b[39m [#########################################################.] \u001b8Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for mime-support (3.64ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.17) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
    "!sudo apt install -y ./google-chrome-stable_current_amd64.deb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->webdriver-manager) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->webdriver-manager) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->webdriver-manager) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "du: cannot access '/teamspace/studios/this_studio/fbinaulimagedataset': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!du -sh /teamspace/studios/this_studio/fbinaulimagedataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 229 links in the file\n",
      "Setting up Chrome driver...\n",
      "Error rotating browser session: Message: <unknown>: Failed to read the 'localStorage' property from 'Window': Storage is disabled inside 'data:' URLs.\n",
      "  (Session info: chrome=134.0.6998.35)\n",
      "Stacktrace:\n",
      "#0 0x55b6872bc46a <unknown>\n",
      "#1 0x55b686d75ed0 <unknown>\n",
      "#2 0x55b686d7ca1e <unknown>\n",
      "#3 0x55b686d7f547 <unknown>\n",
      "#4 0x55b686e14e2b <unknown>\n",
      "#5 0x55b686ded862 <unknown>\n",
      "#6 0x55b686e13ceb <unknown>\n",
      "#7 0x55b686ded633 <unknown>\n",
      "#8 0x55b686db91be <unknown>\n",
      "#9 0x55b686dba981 <unknown>\n",
      "#10 0x55b68728286b <unknown>\n",
      "#11 0x55b68728673c <unknown>\n",
      "#12 0x55b687269f12 <unknown>\n",
      "#13 0x55b6872872b4 <unknown>\n",
      "#14 0x55b68724e0af <unknown>\n",
      "#15 0x55b6872aaad8 <unknown>\n",
      "#16 0x55b6872aacb6 <unknown>\n",
      "#17 0x55b6872bb2e6 <unknown>\n",
      "#18 0x7f340034f609 start_thread\n",
      "\n",
      "Processing 1/229: https://www.facebook.com/100079076260704/posts/643288114983725/?app=fbl\n",
      "Processing URL: https://www.facebook.com/100079076260704/posts/643288114983725/?app=fbl\n",
      "Attempting to load page...\n",
      "Page loaded in 0.81 seconds\n",
      "Waiting 5 seconds for dynamic content...\n",
      "Extracting initial images...\n",
      "Found 8 images in initial page\n",
      "Attempting to click on images...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 544\u001b[0m\n\u001b[1;32m    541\u001b[0m     log_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 544\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 501\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    498\u001b[0m log_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(fb_links)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# Extract high-quality images\u001b[39;00m\n\u001b[0;32m--> 501\u001b[0m high_res_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mget_high_quality_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m high_res_imgs:\n\u001b[1;32m    504\u001b[0m     log_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo high-quality images found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 329\u001b[0m, in \u001b[0;36mget_high_quality_images\u001b[0;34m(driver, post_url, timeout)\u001b[0m\n\u001b[1;32m    327\u001b[0m log_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to click on images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    328\u001b[0m click_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 329\u001b[0m clicked \u001b[38;5;241m=\u001b[39m \u001b[43mclick_on_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m click_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m click_start\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clicked:\n",
      "Cell \u001b[0;32mIn[24], line 275\u001b[0m, in \u001b[0;36mclick_on_image\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments[0].scrollIntoView(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mbehavior: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmooth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, block: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m});\u001b[39m\u001b[38;5;124m\"\u001b[39m, img)\n\u001b[0;32m--> 275\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Allow time for scrolling\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# Try different click methods\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import unquote, urlparse, parse_qs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Configuration settings\n",
    "CONFIG = {\n",
    "    \"wait_time\": 5,  # Time to wait for page elements to load (seconds)\n",
    "    \"save_dir\": \"facebook_images_hd\",  # Default save directory\n",
    "    \"failed_links_file\": \"failed_links.txt\",  # File to save failed links\n",
    "    \"log_file\": \"scraper_log.txt\",  # Log file for debugging\n",
    "    \"max_retries\": 3,  # Maximum number of retries for failed downloads\n",
    "    \"retry_delay\": 2,  # Initial delay between retries (seconds)\n",
    "    \"browser_restart_interval\": 5,  # Restart browser every X posts\n",
    "}\n",
    "\n",
    "def log_message(message, file=CONFIG[\"log_file\"]):\n",
    "    \"\"\"Log a message to both console and log file.\"\"\"\n",
    "    print(message)\n",
    "    with open(file, \"a\") as f:\n",
    "        f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\\n\")\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Setup and return a Chrome webdriver with appropriate options.\"\"\"\n",
    "    log_message(\"Setting up Chrome driver...\")\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    # Use a realistic user agent\n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\"\n",
    "    ]\n",
    "    options.add_argument(f\"--user-agent={random.choice(user_agents)}\")\n",
    "    \n",
    "    # Add more browser-like behavior\n",
    "    options.add_argument(\"--disable-features=IsolateOrigins,site-per-process\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    # Set navigator.webdriver to undefined\n",
    "    driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "        \"source\": \"\"\"\n",
    "        Object.defineProperty(navigator, 'webdriver', {\n",
    "            get: () => undefined\n",
    "        })\n",
    "        \"\"\"\n",
    "    })\n",
    "    \n",
    "    return driver\n",
    "\n",
    "def rotate_browser_session(driver):\n",
    "    \"\"\"Rotate browser session to avoid detection.\"\"\"\n",
    "    try:\n",
    "        # Clear cookies and cache\n",
    "        driver.delete_all_cookies()\n",
    "        driver.execute_script(\"window.localStorage.clear();\")\n",
    "        driver.execute_script(\"window.sessionStorage.clear();\")\n",
    "        \n",
    "        # Randomize window size slightly\n",
    "        width = random.randint(1024, 1920)\n",
    "        height = random.randint(768, 1080)\n",
    "        driver.set_window_size(width, height)\n",
    "        \n",
    "        # Change user agent periodically\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "        ]\n",
    "        driver.execute_cdp_cmd('Network.setUserAgentOverride', {\"userAgent\": random.choice(user_agents)})\n",
    "        \n",
    "        log_message(\"Browser session rotated\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error rotating browser session: {str(e)}\")\n",
    "\n",
    "def create_session():\n",
    "    \"\"\"Create and return a requests session with more browser-like behavior.\"\"\"\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Use the appropriate retry parameter based on requests version\n",
    "    try:\n",
    "        retry_strategy = Retry(\n",
    "            total=CONFIG[\"max_retries\"],\n",
    "            backoff_factor=CONFIG[\"retry_delay\"],\n",
    "            status_forcelist=[429, 500, 502, 503, 504],  # Don't retry on 403\n",
    "            allowed_methods=[\"GET\"]  # New parameter name\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Fall back to older parameter name if needed\n",
    "        retry_strategy = Retry(\n",
    "            total=CONFIG[\"max_retries\"],\n",
    "            backoff_factor=CONFIG[\"retry_delay\"],\n",
    "            status_forcelist=[429, 500, 502, 503, 504],  # Don't retry on 403\n",
    "            method_whitelist=[\"GET\"]  # Old parameter name\n",
    "        )\n",
    "    \n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    \n",
    "    # Add common browser cookies and headers\n",
    "    session.cookies.set(\"locale\", \"en_US\")\n",
    "    session.cookies.set(\"noscript\", \"1\")\n",
    "    session.headers.update({\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Cache-Control\": \"max-age=0\"\n",
    "    })\n",
    "    \n",
    "    return session\n",
    "\n",
    "def clean_url(url):\n",
    "    \"\"\"Clean the URL while preserving necessary parameters.\"\"\"\n",
    "    # Some parameters are actually needed for the URL to work\n",
    "    parsed_url = urlparse(url)\n",
    "    params = parse_qs(parsed_url.query)\n",
    "    \n",
    "    # Only remove tracking parameters, keep those needed for access\n",
    "    parameters_to_keep = ['_nc_cat', 'ccb', '_nc_sid', '_nc_ohc', '_nc_ht']\n",
    "    clean_params = {k: v[0] for k, v in params.items() if k in parameters_to_keep}\n",
    "    \n",
    "    new_query = \"&\".join([f\"{k}={v}\" for k, v in clean_params.items()])\n",
    "    \n",
    "    # Construct the clean URL\n",
    "    clean = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}\"\n",
    "    if new_query:\n",
    "        clean += f\"?{new_query}\"\n",
    "    \n",
    "    return clean\n",
    "\n",
    "def get_alternative_urls(url):\n",
    "    \"\"\"Generate alternative URL formats to try when the main URL fails.\"\"\"\n",
    "    alternatives = []\n",
    "    \n",
    "    # Try different CDN domains\n",
    "    for cdn in [\"scontent.xx\", \"scontent-iad3-1.xx\", \"scontent-iad3-2.xx\", \"scontent-dfw5-1.xx\"]:\n",
    "        alt = re.sub(r'scontent-[^\\.]+\\.xx', cdn, url)\n",
    "        alternatives.append(alt)\n",
    "    \n",
    "    # Try with and without various parameters\n",
    "    parsed = urlparse(url)\n",
    "    params = parse_qs(parsed.query)\n",
    "    \n",
    "    # Generate URLs with different parameter combinations\n",
    "    base = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    # Just the base URL with minimal parameters\n",
    "    if '_nc_cat' in params and '_nc_sid' in params:\n",
    "        minimal = f\"{base}?_nc_cat={params['_nc_cat'][0]}&_nc_sid={params['_nc_sid'][0]}\"\n",
    "        alternatives.append(minimal)\n",
    "    \n",
    "    # Try adding a direct download parameter\n",
    "    alternatives.append(f\"{url}&dl=1\")\n",
    "    \n",
    "    return alternatives\n",
    "\n",
    "def extract_image_urls_from_source(html_source):\n",
    "    \"\"\"Extract high-quality image URLs from HTML source.\"\"\"\n",
    "    soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "    image_urls = set()\n",
    "    \n",
    "    # Method 1: Find all images with scontent in src\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        if img.get(\"src\") and \"scontent\" in img[\"src\"]:\n",
    "            url = clean_url(img[\"src\"])\n",
    "            image_urls.add(url)\n",
    "    \n",
    "    # Method 2: Look for preloaded image links\n",
    "    for link in soup.find_all(\"link\", {\"rel\": \"preload\", \"as\": \"image\"}):\n",
    "        if link.get(\"href\") and \"scontent\" in link[\"href\"]:\n",
    "            url = clean_url(link[\"href\"])\n",
    "            image_urls.add(url)\n",
    "    \n",
    "    # Method 3: Extract from meta tags\n",
    "    for meta in soup.find_all(\"meta\", {\"property\": \"og:image\"}):\n",
    "        if meta.get(\"content\") and \"scontent\" in meta[\"content\"]:\n",
    "            url = clean_url(meta[\"content\"])\n",
    "            image_urls.add(url)\n",
    "    \n",
    "    # Method 4: Look for data attributes with image URLs\n",
    "    for element in soup.find_all(attrs={\"data-visualcompletion\": \"media-vc-image\"}):\n",
    "        if element.get(\"src\") and \"scontent\" in element[\"src\"]:\n",
    "            url = clean_url(element[\"src\"])\n",
    "            image_urls.add(url)\n",
    "    \n",
    "    # Method 5: Extract from background-image style\n",
    "    for element in soup.find_all(style=True):\n",
    "        if \"background-image\" in element[\"style\"] and \"scontent\" in element[\"style\"]:\n",
    "            match = re.search(r'url\\([\\'\"]?(.*?)[\\'\"]?\\)', element[\"style\"])\n",
    "            if match:\n",
    "                url = clean_url(match.group(1))\n",
    "                if \"scontent\" in url:\n",
    "                    image_urls.add(url)\n",
    "    \n",
    "    # Method 6: Extract from data-store attribute (common in Facebook)\n",
    "    for element in soup.find_all(attrs={\"data-store\": True}):\n",
    "        data_store = element.get(\"data-store\")\n",
    "        if data_store and \"scontent\" in data_store:\n",
    "            urls = re.findall(r'https?://[^\"\\']+scontent[^\"\\']+', data_store)\n",
    "            for url in urls:\n",
    "                clean = clean_url(url)\n",
    "                image_urls.add(clean)\n",
    "    \n",
    "    # Method 7: Look for AJAX responses with image data\n",
    "    for script in soup.find_all(\"script\"):\n",
    "        if script.string and \"scontent\" in script.string:\n",
    "            urls = re.findall(r'https?://[^\"\\']+scontent[^\"\\']+', script.string)\n",
    "            for url in urls:\n",
    "                if \"jpg\" in url.lower() or \"jpeg\" in url.lower() or \"png\" in url.lower():\n",
    "                    clean = clean_url(url)\n",
    "                    image_urls.add(clean)\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "def click_on_image(driver):\n",
    "    \"\"\"Try to click on images to get to the full-resolution version.\"\"\"\n",
    "    try:\n",
    "        # Wait for and find clickable images using various selectors\n",
    "        selectors = [\n",
    "            \"img[data-visualcompletion='media-vc-image']\",\n",
    "            \"a[role='link'] img[src*='scontent']\",\n",
    "            \"img[class*='x1bwycvy']\",  # Common Facebook image class\n",
    "            \"img[class*='x168nmei']\",\n",
    "            \"div[role='button'] img[src*='scontent']\",\n",
    "            \"img[src*='scontent']\"\n",
    "        ]\n",
    "        \n",
    "        clicked = False\n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                # Wait for selector to be present\n",
    "                WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                )\n",
    "                \n",
    "                # Find all matching elements\n",
    "                images = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                \n",
    "                if images:\n",
    "                    # Try to click each image\n",
    "                    for img in images:\n",
    "                        try:\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\", img)\n",
    "                            time.sleep(0.5)  # Allow time for scrolling\n",
    "                            \n",
    "                            # Try different click methods\n",
    "                            try:\n",
    "                                img.click()  # Direct click\n",
    "                            except:\n",
    "                                try:\n",
    "                                    driver.execute_script(\"arguments[0].click();\", img)  # JavaScript click\n",
    "                                except:\n",
    "                                    # Try to find and click the parent element\n",
    "                                    parent = driver.execute_script(\"return arguments[0].parentNode;\", img)\n",
    "                                    driver.execute_script(\"arguments[0].click();\", parent)\n",
    "                            \n",
    "                            clicked = True\n",
    "                            time.sleep(2)  # Wait for image to load in full resolution\n",
    "                        except Exception as e:\n",
    "                            log_message(f\"Failed to click image using {selector}: {str(e)}\")\n",
    "                            continue\n",
    "            except TimeoutException:\n",
    "                continue\n",
    "        \n",
    "        return clicked\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error in click_on_image function: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_high_quality_images(driver, post_url, timeout=60):\n",
    "    \"\"\"Extract high-quality images from a Facebook post with timeout.\"\"\"\n",
    "    log_message(f\"Processing URL: {post_url}\")\n",
    "    \n",
    "    # Set a page load timeout\n",
    "    driver.set_page_load_timeout(timeout)\n",
    "    \n",
    "    try:\n",
    "        # Navigate to the post\n",
    "        log_message(\"Attempting to load page...\")\n",
    "        start_time = time.time()\n",
    "        driver.get(post_url)\n",
    "        load_time = time.time() - start_time\n",
    "        log_message(f\"Page loaded in {load_time:.2f} seconds\")\n",
    "        \n",
    "        # Wait for page content\n",
    "        log_message(f\"Waiting {CONFIG['wait_time']} seconds for dynamic content...\")\n",
    "        time.sleep(CONFIG[\"wait_time\"])\n",
    "        \n",
    "        # Get initial image URLs\n",
    "        log_message(\"Extracting initial images...\")\n",
    "        initial_html = driver.page_source\n",
    "        initial_images = extract_image_urls_from_source(initial_html)\n",
    "        log_message(f\"Found {len(initial_images)} images in initial page\")\n",
    "        \n",
    "        # Try to click on images to get full resolution versions\n",
    "        log_message(\"Attempting to click on images...\")\n",
    "        click_start = time.time()\n",
    "        clicked = click_on_image(driver)\n",
    "        click_time = time.time() - click_start\n",
    "        \n",
    "        if clicked:\n",
    "            log_message(f\"Successfully clicked on images in {click_time:.2f} seconds, extracting high-res versions\")\n",
    "            time.sleep(3)  # Wait longer for modal popup\n",
    "            \n",
    "            # Get image URLs after clicking\n",
    "            log_message(\"Extracting images after clicking...\")\n",
    "            after_click_html = driver.page_source\n",
    "            after_click_images = extract_image_urls_from_source(after_click_html)\n",
    "            log_message(f\"Found {len(after_click_images)} images after clicking\")\n",
    "        else:\n",
    "            log_message(f\"No images clicked after {click_time:.2f} seconds, using initial image URLs only\")\n",
    "            after_click_images = []\n",
    "        \n",
    "        # Combine and deduplicate the image URLs\n",
    "        all_image_urls = list(set(initial_images + after_click_images))\n",
    "        \n",
    "        # Apply additional URL modifications to ensure highest resolution\n",
    "        high_res_urls = []\n",
    "        for url in all_image_urls:\n",
    "            high_res_url = clean_url(url)\n",
    "            high_res_urls.append(high_res_url)\n",
    "        \n",
    "        log_message(f\"Found {len(high_res_urls)} high-quality images\")\n",
    "        return list(set(high_res_urls))  # Remove duplicates\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error processing {post_url}: {str(e)}\")\n",
    "        # Force continue even if there's an error\n",
    "        driver.execute_script(\"window.stop();\")\n",
    "        return []\n",
    "\n",
    "def download_image(session, url, folder, post_index, img_index):\n",
    "    \"\"\"Download an image with improved error handling.\"\"\"\n",
    "    try:\n",
    "        # Prepare filename\n",
    "        if \"?\" in url:\n",
    "            base_name = url.split(\"?\")[0].split(\"/\")[-1]\n",
    "        else:\n",
    "            base_name = url.split(\"/\")[-1]\n",
    "        \n",
    "        # Ensure the base name has a proper extension\n",
    "        if not (base_name.lower().endswith('.jpg') or base_name.lower().endswith('.jpeg') or \n",
    "                base_name.lower().endswith('.png') or base_name.lower().endswith('.gif')):\n",
    "            base_name += \".jpg\"  # Default to jpg if no extension\n",
    "        \n",
    "        filename = os.path.join(folder, f\"post{post_index}_img{img_index}_{base_name}\")\n",
    "        \n",
    "        # Try the original URL first, then alternatives if needed\n",
    "        urls_to_try = [url] + get_alternative_urls(url)\n",
    "        \n",
    "        for current_url_index, current_url in enumerate(urls_to_try):\n",
    "            try:\n",
    "                # Add a delay to avoid rate limiting\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "                \n",
    "                # Rotate user agents\n",
    "                user_agents = [\n",
    "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "                    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15\",\n",
    "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "                ]\n",
    "                \n",
    "                headers = {\n",
    "                    \"User-Agent\": random.choice(user_agents),\n",
    "                    \"Referer\": \"https://www.facebook.com/\",\n",
    "                    \"Accept\": \"image/avif,image/webp,image/apng,image/*,*/*;q=0.8\",\n",
    "                    \"Origin\": \"https://www.facebook.com\",\n",
    "                    \"sec-ch-ua\": '\"Chromium\";v=\"92\", \" Not A;Brand\";v=\"99\", \"Google Chrome\";v=\"92\"'\n",
    "                }\n",
    "                \n",
    "                # Add randomness to headers to avoid detection\n",
    "                if random.random() > 0.5:\n",
    "                    headers[\"Accept-Encoding\"] = \"gzip, deflate, br\"\n",
    "                if random.random() > 0.5:\n",
    "                    headers[\"sec-ch-ua-mobile\"] = \"?0\"\n",
    "                \n",
    "                log_message(f\"Trying URL {current_url_index + 1}/{len(urls_to_try)}: {current_url}\")\n",
    "                \n",
    "                response = session.get(current_url, stream=True, headers=headers, timeout=15)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Check content type to ensure it's an image\n",
    "                    content_type = response.headers.get('Content-Type', '')\n",
    "                    if not content_type.startswith('image/'):\n",
    "                        log_message(f\"Warning: Content-Type is {content_type}, not an image\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Download the image\n",
    "                    total_size = int(response.headers.get('content-length', 0))\n",
    "                    with open(filename, \"wb\") as file, tqdm(\n",
    "                        desc=f\"Downloading {os.path.basename(filename)}\",\n",
    "                        total=total_size,\n",
    "                        unit='B',\n",
    "                        unit_scale=True,\n",
    "                        unit_divisor=1024,\n",
    "                    ) as bar:\n",
    "                        for chunk in response.iter_content(chunk_size=1024):\n",
    "                            size = file.write(chunk)\n",
    "                            bar.update(size)\n",
    "                    \n",
    "                    # Verify file integrity\n",
    "                    if os.path.exists(filename) and os.path.getsize(filename) > 1000:  # Basic size check\n",
    "                        log_message(f\"Successfully downloaded: {filename}\")\n",
    "                        return True\n",
    "                    else:\n",
    "                        log_message(f\"Downloaded file too small, may be corrupted\")\n",
    "                        if os.path.exists(filename):\n",
    "                            os.remove(filename)\n",
    "                \n",
    "                elif response.status_code == 403:\n",
    "                    log_message(f\"Access forbidden (403) for URL variant {current_url_index + 1}, trying next alternative\")\n",
    "                \n",
    "                else:\n",
    "                    log_message(f\"HTTP status {response.status_code} for URL variant {current_url_index + 1}\")\n",
    "                \n",
    "            except requests.exceptions.RequestException as req_err:\n",
    "                log_message(f\"Request error for URL variant {current_url_index + 1}: {str(req_err)}\")\n",
    "        \n",
    "        log_message(f\"Failed to download image after trying all URL alternatives\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Fatal error downloading image: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the Facebook image scraper.\"\"\"\n",
    "    # Clear log file\n",
    "    open(CONFIG[\"log_file\"], 'w').close()\n",
    "    \n",
    "    # Prompt user for input file\n",
    "    file_path = input(\"Enter the full path of the .txt file containing Facebook post links: \")\n",
    "    \n",
    "    # Read links from file\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            fb_links = [line.strip() for line in file if line.strip().startswith(\"http\")]\n",
    "        log_message(f\"Found {len(fb_links)} links in the file\")\n",
    "    except FileNotFoundError:\n",
    "        log_message(\"File not found!\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(CONFIG[\"save_dir\"], exist_ok=True)\n",
    "    \n",
    "    # Initialize driver and session\n",
    "    driver = setup_driver()\n",
    "    session = create_session()\n",
    "    \n",
    "    processed = 0\n",
    "    failed_links = []\n",
    "    \n",
    "    # Process each link\n",
    "    for i, link in enumerate(fb_links):\n",
    "        try:\n",
    "            # Restart browser periodically to avoid detection patterns\n",
    "            if i > 0 and i % CONFIG[\"browser_restart_interval\"] == 0:\n",
    "                log_message(\"Restarting browser session...\")\n",
    "                driver.quit()\n",
    "                time.sleep(5)  # Wait before restarting\n",
    "                driver = setup_driver()\n",
    "            else:\n",
    "                # Just rotate the browser session for freshness\n",
    "                rotate_browser_session(driver)\n",
    "            \n",
    "            processed += 1\n",
    "            log_message(f\"Processing {processed}/{len(fb_links)}: {link}\")\n",
    "            \n",
    "            # Extract high-quality images\n",
    "            high_res_imgs = get_high_quality_images(driver, link)\n",
    "            \n",
    "            if not high_res_imgs:\n",
    "                log_message(f\"No high-quality images found in {link}\")\n",
    "                failed_links.append(link)\n",
    "                continue\n",
    "            \n",
    "            # Download each image\n",
    "            successful_downloads = 0\n",
    "            for j, img_url in enumerate(high_res_imgs):\n",
    "                log_message(f\"Downloading image {j+1}/{len(high_res_imgs)}\")\n",
    "                success = download_image(session, img_url, CONFIG[\"save_dir\"], i+1, j+1)\n",
    "                if success:\n",
    "                    successful_downloads += 1\n",
    "            \n",
    "            log_message(f\"Successfully downloaded {successful_downloads}/{len(high_res_imgs)} images from post {i+1}\")\n",
    "            \n",
    "            # If no successful downloads, add to failed links\n",
    "            if successful_downloads == 0 and len(high_res_imgs) > 0:\n",
    "                failed_links.append(link)\n",
    "            \n",
    "            # Pause between posts with random delay to avoid being blocked\n",
    "            pause_time = random.uniform(3, 8)\n",
    "            log_message(f\"Pausing for {pause_time:.2f} seconds before next post\")\n",
    "            time.sleep(pause_time)\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_message(f\"Failed to process {link}: {str(e)}\")\n",
    "            failed_links.append(link)\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    \n",
    "    # Save failed links\n",
    "    if failed_links:\n",
    "        with open(CONFIG[\"failed_links_file\"], \"w\") as f:\n",
    "            for fl in failed_links:\n",
    "                f.write(fl + \"\\n\")\n",
    "        log_message(f\"Failed links saved in {CONFIG['failed_links_file']}\")\n",
    "    \n",
    "    log_message(\"Process completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--url URL] [--urls-file URLS_FILE]\n",
      "                             [--output OUTPUT] [--headless] [--login]\n",
      "                             [--wait-time WAIT_TIME] [--post-count POST_COUNT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/teamspace/studios/this_studio/.local/share/jupyter/runtime/kernel-v2-9645Gb8ozetmCwU.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3556: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentException",
     "evalue": "Message: invalid argument\n  (Session info: chrome=134.0.6998.35)\nStacktrace:\n#0 0x5638751c346a <unknown>\n#1 0x563874c7cd23 <unknown>\n#2 0x563874c6391c <unknown>\n#3 0x563874c61f2d <unknown>\n#4 0x563874c6257a <unknown>\n#5 0x563874c80369 <unknown>\n#6 0x563874d1b9f5 <unknown>\n#7 0x563874cf4862 <unknown>\n#8 0x563874d1aceb <unknown>\n#9 0x563874cf4633 <unknown>\n#10 0x563874cc01be <unknown>\n#11 0x563874cc1981 <unknown>\n#12 0x56387518986b <unknown>\n#13 0x56387518d73c <unknown>\n#14 0x563875170f12 <unknown>\n#15 0x56387518e2b4 <unknown>\n#16 0x5638751550af <unknown>\n#17 0x5638751b1ad8 <unknown>\n#18 0x5638751b1cb6 <unknown>\n#19 0x5638751c22e6 <unknown>\n#20 0x7f3f3f73b609 start_thread\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys\u001b[38;5;241m.\u001b[39margv) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     66\u001b[0m     post_url \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mscrape_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpost_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsage: python Facebook_Image_Scraper.py <Facebook Post URL>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 27\u001b[0m, in \u001b[0;36mscrape_images\u001b[0;34m(url, output_dir)\u001b[0m\n\u001b[1;32m     24\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir)\n\u001b[1;32m     26\u001b[0m driver \u001b[38;5;241m=\u001b[39m init_driver()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Wait until at least one high-res image appears. Adjust the timeout as needed.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:454\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m    tab.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mInvalidArgumentException\u001b[0m: Message: invalid argument\n  (Session info: chrome=134.0.6998.35)\nStacktrace:\n#0 0x5638751c346a <unknown>\n#1 0x563874c7cd23 <unknown>\n#2 0x563874c6391c <unknown>\n#3 0x563874c61f2d <unknown>\n#4 0x563874c6257a <unknown>\n#5 0x563874c80369 <unknown>\n#6 0x563874d1b9f5 <unknown>\n#7 0x563874cf4862 <unknown>\n#8 0x563874d1aceb <unknown>\n#9 0x563874cf4633 <unknown>\n#10 0x563874cc01be <unknown>\n#11 0x563874cc1981 <unknown>\n#12 0x56387518986b <unknown>\n#13 0x56387518d73c <unknown>\n#14 0x563875170f12 <unknown>\n#15 0x56387518e2b4 <unknown>\n#16 0x5638751550af <unknown>\n#17 0x5638751b1ad8 <unknown>\n#18 0x5638751b1cb6 <unknown>\n#19 0x5638751c22e6 <unknown>\n#20 0x7f3f3f73b609 start_thread\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    # (Optional) reduce logging\n",
    "    chrome_options.add_argument(\"--log-level=3\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def scrape_images(url, output_dir=\"images\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    driver = init_driver()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait until at least one high-res image appears. Adjust the timeout as needed.\n",
    "    try:\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"img[data-visualcompletion='media-vc-image']\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error waiting for images to load: {e}\")\n",
    "        driver.quit()\n",
    "        return\n",
    "\n",
    "    # Optionally scroll down to trigger lazy loading.\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Allow extra time for images to load\n",
    "\n",
    "    # Find all high-resolution image elements (the high-res version typically has this attribute)\n",
    "    image_elements = driver.find_elements(By.CSS_SELECTOR, \"img[data-visualcompletion='media-vc-image']\")\n",
    "    print(f\"Found {len(image_elements)} image element(s).\")\n",
    "\n",
    "    downloaded = 0\n",
    "    for index, img in enumerate(image_elements):\n",
    "        src = img.get_attribute(\"src\")\n",
    "        if src:\n",
    "            print(f\"Downloading image {index+1}: {src}\")\n",
    "            try:\n",
    "                img_data = requests.get(src).content\n",
    "                filename = os.path.join(output_dir, f\"image_{index+1}.jpg\")\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(img_data)\n",
    "                downloaded += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download image {index+1}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"Downloaded {downloaded} image(s) to '{output_dir}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) > 1:\n",
    "        post_url = sys.argv[1]\n",
    "        scrape_images(post_url)\n",
    "    else:\n",
    "        print(\"Usage: python Facebook_Image_Scraper.py <Facebook Post URL>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image as PILImage\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "\n",
    "class FacebookImageScraper:\n",
    "    def __init__(self):\n",
    "        self.output_folder = \"Facebook Images Scraped\"\n",
    "        self.failed_links_file = \"failed_links.txt\"\n",
    "        self.timeout = 30\n",
    "        self.driver = None\n",
    "        self.session = None\n",
    "        self.use_login = False\n",
    "        self.email = None\n",
    "        self.password = None\n",
    "        self.headless = False\n",
    "        \n",
    "        # Create output directory\n",
    "        if not os.path.exists(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "            print(f\"Created output directory: {self.output_folder}\")\n",
    "        else:\n",
    "            print(f\"Using existing output directory: {self.output_folder}\")\n",
    "    \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Set up and return a Selenium WebDriver\"\"\"\n",
    "        print(\"Setting up browser...\")\n",
    "        chrome_options = Options()\n",
    "        if self.headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--disable-notifications\")\n",
    "        chrome_options.add_argument(\"--disable-infobars\")\n",
    "        chrome_options.add_argument(\"--start-maximized\")\n",
    "        chrome_options.add_argument(\"--disable-extensions\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        \n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        return self.driver\n",
    "    \n",
    "    def setup_session(self):\n",
    "        \"\"\"Set up and return a requests Session\"\"\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Referer': 'https://www.facebook.com/',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "        }\n",
    "        \n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(headers)\n",
    "        return self.session\n",
    "    \n",
    "    def login_to_facebook(self):\n",
    "        \"\"\"Login to Facebook using Selenium\"\"\"\n",
    "        if not self.use_login or not self.email or not self.password:\n",
    "            print(\"Proceeding without login. Some content may not be accessible.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"Logging into Facebook...\")\n",
    "        self.driver.get(\"https://www.facebook.com/\")\n",
    "        \n",
    "        # Accept cookies if prompted\n",
    "        try:\n",
    "            cookie_button = WebDriverWait(self.driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(string(), 'Allow') or contains(string(), 'Accept')]\"))\n",
    "            )\n",
    "            cookie_button.click()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            pass\n",
    "            \n",
    "        # Enter email\n",
    "        try:\n",
    "            email_field = WebDriverWait(self.driver, self.timeout).until(\n",
    "                EC.presence_of_element_located((By.ID, \"email\"))\n",
    "            )\n",
    "            email_field.send_keys(self.email)\n",
    "            \n",
    "            # Enter password\n",
    "            password_field = self.driver.find_element(By.ID, \"pass\")\n",
    "            password_field.send_keys(self.password)\n",
    "            \n",
    "            # Click login\n",
    "            login_button = self.driver.find_element(By.NAME, \"login\")\n",
    "            login_button.click()\n",
    "            \n",
    "            # Wait for login to complete\n",
    "            try:\n",
    "                WebDriverWait(self.driver, self.timeout).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"//div[@role='navigation']\"))\n",
    "                )\n",
    "                print(\"Login successful\")\n",
    "                \n",
    "                # Pass the cookies from Selenium to Requests session\n",
    "                selenium_cookies = self.driver.get_cookies()\n",
    "                for cookie in selenium_cookies:\n",
    "                    self.session.cookies.set(cookie['name'], cookie['value'], domain=cookie['domain'])\n",
    "                \n",
    "                return True\n",
    "            except TimeoutException:\n",
    "                print(\"Login may have failed or page structure has changed. Continuing anyway...\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error during login: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def get_links_from_file(self, file_path):\n",
    "        \"\"\"Extract Facebook links from a text file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                links = []\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if line and ('facebook.com' in line or 'fb.com' in line):\n",
    "                        links.append(line)\n",
    "                \n",
    "                print(f\"Found {len(links)} Facebook links in file\")\n",
    "                return links\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_image_urls_selenium(self, post_url):\n",
    "        \"\"\"Extract image URLs from a Facebook post using Selenium\"\"\"\n",
    "        if not self.driver:\n",
    "            self.setup_driver()\n",
    "            \n",
    "        # Navigate to the post URL\n",
    "        print(f\"Navigating to: {post_url}\")\n",
    "        self.driver.get(post_url)\n",
    "        time.sleep(5)  # Allow time for the page to load\n",
    "        \n",
    "        # First get the visible image links as a fallback\n",
    "        visible_image_links = []\n",
    "        try:\n",
    "            # Multiple XPath patterns to find visible image links\n",
    "            xpath_patterns = [\n",
    "                \"//a[contains(@href, '/photo/?fbid=')]\",\n",
    "                \"//a[contains(@href, '/photo?fbid=')]\",\n",
    "                \"//a[contains(@href, 'set=pcb.')]\",\n",
    "                \"//div[@role='article']//a[.//img]\"\n",
    "            ]\n",
    "            \n",
    "            for pattern in xpath_patterns:\n",
    "                elements = self.driver.find_elements(By.XPATH, pattern)\n",
    "                for element in elements:\n",
    "                    link = element.get_attribute('href')\n",
    "                    if link and link not in visible_image_links and ('fbid=' in link or 'photo' in link):\n",
    "                        visible_image_links.append(link)\n",
    "            \n",
    "            if visible_image_links:\n",
    "                print(f\"Found {len(visible_image_links)} visible image links\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding visible image links: {str(e)}\")\n",
    "        \n",
    "        # Primary approach: Click on the first image and navigate through all photos\n",
    "        image_links = []\n",
    "        viewer_opened = False\n",
    "        \n",
    "        try:\n",
    "            # Multiple attempts to find clickable images\n",
    "            clickable_selectors = [\n",
    "                \"//div[@role='article']//img[not(contains(@src, 'emoji')) and not(contains(@src, 'icon'))]\",\n",
    "                \"//div[contains(@class, 'userContent')]//img[not(contains(@src, 'emoji'))]\",\n",
    "                \"//div[contains(@class, 'story')]//img[not(contains(@src, 'emoji'))]\",\n",
    "                \"//a[contains(@href, '/photo')]//img\",\n",
    "                \"//div[@role='main']//img[not(contains(@src, 'emoji')) and not(contains(@src, 'icon')) and not(contains(@src, 'gif')) and not(contains(@src, 'png'))]\"\n",
    "            ]\n",
    "            \n",
    "            for selector in clickable_selectors:\n",
    "                try:\n",
    "                    images = self.driver.find_elements(By.XPATH, selector)\n",
    "                    \n",
    "                    if images:\n",
    "                        print(f\"Found {len(images)} potential clickable images\")\n",
    "                        # Try each image until one opens the viewer\n",
    "                        for i, img in enumerate(images[:5]):  # Try first 5 images maximum\n",
    "                            try:\n",
    "                                self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", img)\n",
    "                                time.sleep(1)\n",
    "                                img.click()\n",
    "                                time.sleep(3)  # Wait for viewer to open\n",
    "                                \n",
    "                                # Check if we're in a photo viewer\n",
    "                                try:\n",
    "                                    viewer_element = WebDriverWait(self.driver, 5).until(\n",
    "                                        EC.presence_of_element_located((By.XPATH, \"//div[@aria-label='Photo viewer']\"))\n",
    "                                    )\n",
    "                                    print(\"Photo viewer opened successfully\")\n",
    "                                    viewer_opened = True\n",
    "                                    break\n",
    "                                except:\n",
    "                                    # Try another way to detect the viewer\n",
    "                                    current_url = self.driver.current_url\n",
    "                                    if 'photo' in current_url and 'fbid=' in current_url:\n",
    "                                        print(\"Photo viewer detected through URL\")\n",
    "                                        viewer_opened = True\n",
    "                                        break\n",
    "                                    else:\n",
    "                                        print(\"Click did not open photo viewer, trying to go back...\")\n",
    "                                        self.driver.back()\n",
    "                                        time.sleep(2)\n",
    "                            except Exception as e:\n",
    "                                # Try to recover\n",
    "                                try:\n",
    "                                    self.driver.back()\n",
    "                                    time.sleep(2)\n",
    "                                except:\n",
    "                                    pass\n",
    "                                \n",
    "                        if viewer_opened:\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            \n",
    "            # If we successfully opened the viewer, extract all images\n",
    "            if viewer_opened:\n",
    "                print(\"Navigating through all photos in the viewer...\")\n",
    "                \n",
    "                # Record the first image URL\n",
    "                current_url = self.driver.current_url\n",
    "                if 'fbid=' in current_url and current_url not in image_links:\n",
    "                    image_links.append(current_url)\n",
    "                \n",
    "                # Navigate through all photos\n",
    "                photo_count = 1\n",
    "                max_attempts = 30  # Set a reasonable maximum\n",
    "                consecutive_failures = 0\n",
    "                \n",
    "                while photo_count < max_attempts and consecutive_failures < 3:\n",
    "                    try:\n",
    "                        # Find the next button using multiple possible selectors\n",
    "                        next_selectors = [\n",
    "                            \"//div[@aria-label='Next photo']\",\n",
    "                            \"//div[contains(@aria-label, 'Next')]\",\n",
    "                            \"//div[@data-testid='chevron-right-overlay']\",\n",
    "                            \"//div[contains(@class, 'next')]//i\",\n",
    "                            \"//button[contains(@class, 'next')]\"\n",
    "                        ]\n",
    "                        \n",
    "                        next_button = None\n",
    "                        for selector in next_selectors:\n",
    "                            try:\n",
    "                                elements = self.driver.find_elements(By.XPATH, selector)\n",
    "                                if elements:\n",
    "                                    next_button = elements[0]\n",
    "                                    break\n",
    "                            except:\n",
    "                                continue\n",
    "                        \n",
    "                        if next_button:\n",
    "                            self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                            self.driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                            time.sleep(1.5)  # Wait for the next photo to load\n",
    "                            \n",
    "                            # Get the URL of this photo\n",
    "                            current_url = self.driver.current_url\n",
    "                            if 'fbid=' in current_url and current_url not in image_links:\n",
    "                                image_links.append(current_url)\n",
    "                                photo_count += 1\n",
    "                                consecutive_failures = 0\n",
    "                            else:\n",
    "                                consecutive_failures += 1\n",
    "                        else:\n",
    "                            consecutive_failures += 1\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        consecutive_failures += 1\n",
    "                    \n",
    "                    # If multiple failures, we've probably reached the end\n",
    "                    if consecutive_failures >= 3:\n",
    "                        break\n",
    "                \n",
    "                print(f\"Found total of {len(image_links)} images through photo viewer\")\n",
    "                \n",
    "                # Return to the post page\n",
    "                try:\n",
    "                    self.driver.execute_script(\"window.history.go(-{})\".format(len(image_links)))\n",
    "                    time.sleep(2)\n",
    "                except:\n",
    "                    # If history navigation fails, just go back to original URL\n",
    "                    self.driver.get(post_url)\n",
    "                    time.sleep(3)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in photo viewer navigation: {str(e)}\")\n",
    "        \n",
    "        # If photo viewer approach failed, use the fallback visible links\n",
    "        if not image_links and visible_image_links:\n",
    "            print(\"Using fallback visible image links\")\n",
    "            image_links = visible_image_links\n",
    "        \n",
    "        # Clean up URLs - strip tracking parameters while keeping the essential fbid\n",
    "        cleaned_links = []\n",
    "        for url in image_links:\n",
    "            try:\n",
    "                # Extract the essential parts (fbid and set parameters)\n",
    "                if 'fbid=' in url:\n",
    "                    # Find fbid parameter\n",
    "                    fbid_start = url.find('fbid=')\n",
    "                    fbid_end = url.find('&', fbid_start)\n",
    "                    if fbid_end == -1:\n",
    "                        fbid_end = len(url)\n",
    "                    fbid_param = url[fbid_start:fbid_end]\n",
    "                    \n",
    "                    # Try to find set parameter if it exists\n",
    "                    set_param = \"\"\n",
    "                    if 'set=' in url:\n",
    "                        set_start = url.find('set=')\n",
    "                        set_end = url.find('&', set_start)\n",
    "                        if set_end == -1:\n",
    "                            set_end = len(url)\n",
    "                        set_param = \"&\" + url[set_start:set_end]\n",
    "                    \n",
    "                    # Create clean URL\n",
    "                    clean_url = f\"https://www.facebook.com/photo?{fbid_param}{set_param}\"\n",
    "                    if clean_url not in cleaned_links:\n",
    "                        cleaned_links.append(clean_url)\n",
    "                else:\n",
    "                    # Just keep the original URL if we can't parse it properly\n",
    "                    if url not in cleaned_links:\n",
    "                        cleaned_links.append(url)\n",
    "            except Exception as e:\n",
    "                print(f\"Error cleaning URL {url}: {str(e)}\")\n",
    "                if url not in cleaned_links:\n",
    "                    cleaned_links.append(url)\n",
    "        \n",
    "        return cleaned_links\n",
    "    \n",
    "    def download_high_quality_images(self, image_urls, post_url):\n",
    "        \"\"\"Download high-quality images from a list of image URLs\"\"\"\n",
    "        if not self.session:\n",
    "            self.setup_session()\n",
    "        \n",
    "        downloaded_files = []\n",
    "        failed_urls = []\n",
    "        \n",
    "        # Create a subfolder for this post\n",
    "        parsed_url = urlparse(post_url)\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        \n",
    "        # Try to extract a meaningful folder name from the URL\n",
    "        folder_name = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        if 'fbid' in query_params:\n",
    "            folder_name = f\"post_{query_params['fbid'][0]}\"\n",
    "        elif parsed_url.path:\n",
    "            clean_path = re.sub(r'[^\\w\\-_]', '_', parsed_url.path)\n",
    "            folder_name = f\"post_{clean_path[-20:]}\"\n",
    "            \n",
    "        post_folder = os.path.join(self.output_folder, folder_name)\n",
    "        if not os.path.exists(post_folder):\n",
    "            os.makedirs(post_folder)\n",
    "        \n",
    "        # Use tqdm for a progress bar\n",
    "        print(f\"Downloading {len(image_urls)} images from post...\")\n",
    "        for i, url in enumerate(tqdm(image_urls, desc=\"Downloading images\", unit=\"img\")):\n",
    "            try:\n",
    "                # Get the image viewing page\n",
    "                img_page_response = self.session.get(url)\n",
    "                img_page_response.raise_for_status()\n",
    "                \n",
    "                # Parse the page to find the high-quality image\n",
    "                soup = BeautifulSoup(img_page_response.text, 'html.parser')\n",
    "                \n",
    "                # Look for image URLs in the page source using regex\n",
    "                image_pattern = r'https:\\/\\/scontent[^\"\\']+\\.(?:jpg|jpeg|png|gif)'\n",
    "                found_urls = re.findall(image_pattern, img_page_response.text)\n",
    "                \n",
    "                # Filter out small images, icons, etc.\n",
    "                high_quality_urls = [u for u in found_urls if not ('icon' in u.lower() or 'emoji' in u.lower())]\n",
    "                \n",
    "                # Remove duplicates while preserving order\n",
    "                high_quality_urls = list(dict.fromkeys(high_quality_urls))\n",
    "                \n",
    "                # If we found high-quality images, download the largest one\n",
    "                if high_quality_urls:\n",
    "                    # Generate a filename with index\n",
    "                    filename = f\"image_{i+1}.jpg\"\n",
    "                    file_path = os.path.join(post_folder, filename)\n",
    "                    \n",
    "                    # Find the largest image (often the first in the list)\n",
    "                    best_url = high_quality_urls[0]\n",
    "                    \n",
    "                    # Try to get actual image size from headers\n",
    "                    best_size = 0\n",
    "                    for img_url in high_quality_urls[:3]:  # Check first few to save time\n",
    "                        try:\n",
    "                            head_response = self.session.head(img_url, timeout=5)\n",
    "                            size = int(head_response.headers.get('Content-Length', 0))\n",
    "                            if size > best_size:\n",
    "                                best_size = size\n",
    "                                best_url = img_url\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Download the best image\n",
    "                    img_response = self.session.get(best_url, stream=True)\n",
    "                    img_response.raise_for_status()\n",
    "                    \n",
    "                    # Save the file\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        for chunk in img_response.iter_content(chunk_size=8192):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                    \n",
    "                    downloaded_files.append(file_path)\n",
    "                else:\n",
    "                    # Fall back to looking for image elements\n",
    "                    images = soup.find_all('img', attrs={'data-visualcompletion': 'media-vc-image'})\n",
    "                    if not images:\n",
    "                        # Try any large image\n",
    "                        images = [img for img in soup.find_all('img') \n",
    "                                 if img.get('width', '0').isdigit() and int(img.get('width', '0')) > 300]\n",
    "                    \n",
    "                    if images:\n",
    "                        src = images[0].get('src')\n",
    "                        # Clean the URL\n",
    "                        clean_url = src.replace('&amp;', '&')\n",
    "                        \n",
    "                        # Generate a filename with index\n",
    "                        filename = f\"image_{i+1}.jpg\"\n",
    "                        file_path = os.path.join(post_folder, filename)\n",
    "                        \n",
    "                        # Download the image\n",
    "                        img_response = self.session.get(clean_url, stream=True)\n",
    "                        img_response.raise_for_status()\n",
    "                        \n",
    "                        # Save the file\n",
    "                        with open(file_path, 'wb') as f:\n",
    "                            for chunk in img_response.iter_content(chunk_size=8192):\n",
    "                                if chunk:\n",
    "                                    f.write(chunk)\n",
    "                        \n",
    "                        downloaded_files.append(file_path)\n",
    "                    else:\n",
    "                        failed_urls.append(url)\n",
    "                        print(f\"No high-quality image found for URL: {url}\")\n",
    "            except Exception as e:\n",
    "                failed_urls.append(url)\n",
    "                print(f\"Error downloading image from {url}: {str(e)}\")\n",
    "        \n",
    "        return downloaded_files, failed_urls\n",
    "    \n",
    "    def process_post(self, post_url):\n",
    "        \"\"\"Process a single Facebook post\"\"\"\n",
    "        try:\n",
    "            # Extract image URLs using Selenium\n",
    "            image_urls = self.extract_image_urls_selenium(post_url)\n",
    "            \n",
    "            if not image_urls:\n",
    "                print(f\"No images found in post: {post_url}\")\n",
    "                return [], [post_url]\n",
    "            \n",
    "            # Download high-quality images\n",
    "            downloaded_files, failed_urls = self.download_high_quality_images(image_urls, post_url)\n",
    "            \n",
    "            return downloaded_files, failed_urls\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing post {post_url}: {str(e)}\")\n",
    "            return [], [post_url]\n",
    "    \n",
    "    def process_links(self, links):\n",
    "        \"\"\"Process a list of Facebook post links\"\"\"\n",
    "        all_downloaded_files = []\n",
    "        all_failed_links = []\n",
    "        \n",
    "        # Set up the session and driver\n",
    "        self.setup_session()\n",
    "        self.setup_driver()\n",
    "        \n",
    "        # Login if needed\n",
    "        if self.use_login:\n",
    "            self.login_to_facebook()\n",
    "        \n",
    "        # Process each link with a progress bar\n",
    "        print(f\"Processing {len(links)} links...\")\n",
    "        for i, link in enumerate(tqdm(links, desc=\"Processing links\", unit=\"link\")):\n",
    "            print(f\"\\nProcessing link {i+1}/{len(links)}: {link}\")\n",
    "            downloaded_files, failed_urls = self.process_post(link)\n",
    "            \n",
    "            all_downloaded_files.extend(downloaded_files)\n",
    "            all_failed_links.extend(failed_urls)\n",
    "            \n",
    "            # Optional sleep to avoid rate limiting\n",
    "            if i < len(links) - 1:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # Log failed links\n",
    "        if all_failed_links:\n",
    "            with open(self.failed_links_file, 'w') as f:\n",
    "                for link in all_failed_links:\n",
    "                    f.write(f\"{link}\\n\")\n",
    "            print(f\"Logged {len(all_failed_links)} failed links to {self.failed_links_file}\")\n",
    "        \n",
    "        # Clean up\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "        \n",
    "        return all_downloaded_files, all_failed_links\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the Facebook Image Scraper with user input\"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Facebook Image Scraper\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Get input from user\n",
    "        input_path = input(\"Enter a Facebook post URL or path to a .txt file containing links: \").strip()\n",
    "        \n",
    "        # Determine if it's a file or a direct URL\n",
    "        links = []\n",
    "        if input_path.endswith('.txt') and os.path.exists(input_path):\n",
    "            links = self.get_links_from_file(input_path)\n",
    "            if not links:\n",
    "                print(\"No valid Facebook links found in the file.\")\n",
    "                return\n",
    "        elif 'facebook.com' in input_path or 'fb.com' in input_path:\n",
    "            links = [input_path]\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter a valid Facebook URL or path to a .txt file.\")\n",
    "            return\n",
    "        \n",
    "        # Ask for login credentials\n",
    "        self.use_login = input(\"Do you want to login to Facebook for better access? (y/n): \").lower() == 'y'\n",
    "        if self.use_login:\n",
    "            self.email = input(\"Enter your Facebook email: \")\n",
    "            self.password = input(\"Enter your Facebook password: \")\n",
    "        \n",
    "        # Ask for headless mode\n",
    "        self.headless = input(\"Run in headless mode (no visible browser)? (y/n): \").lower() == 'y'\n",
    "        \n",
    "        # Process the links\n",
    "        start_time = time.time()\n",
    "        downloaded_files, failed_links = self.process_links(links)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Scraping Summary\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total links processed: {len(links)}\")\n",
    "        print(f\"Total images downloaded: {len(downloaded_files)}\")\n",
    "        print(f\"Failed links: {len(failed_links)}\")\n",
    "        print(f\"Images saved to: {os.path.abspath(self.output_folder)}\")\n",
    "        print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        if failed_links:\n",
    "            print(f\"Failed links saved to: {os.path.abspath(self.failed_links_file)}\")\n",
    "        \n",
    "        print(\"\\nDone!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = FacebookImageScraper()\n",
    "    scraper.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
